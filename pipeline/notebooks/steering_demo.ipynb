{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering demo (corrigibility vs. non-corrigibility)\n",
    "\n",
    "Load a stored persona vector, pick layer/strength, and generate a steered answer.\n",
    "\n",
    "Prereqs:\n",
    "- `OPENAI_API_KEY` if you plan to re-run the judging pipeline elsewhere.\n",
    "- Persona vectors produced by `pipeline.py` (e.g., `pipeline_outputs/persona_vector.pt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]  # pipeline/\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from src.activation_steer import multi_steerer  # local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330db2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "# Configure\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # HF model to steer\n",
    "vector_path = Path(\"../pipeline_outputs/persona_vector.pt\")\n",
    "layers_to_steer = [15]  # vector index (embedding is 0); edit this list freely\n",
    "coeff = 2.0             # steering strength applied to each listed layer\n",
    "positions = \"all\"       # all | prompt | response\n",
    "max_new_tokens = 200\n",
    "temperature = 0.7\n",
    "top_p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model/tokenizer and persona vector\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", token=hf_token)\n",
    "\n",
    "persona_vectors = torch.load(vector_path, map_location=model.device)\n",
    "steering_block_indices = [layer - 1 for layer in layers_to_steer]  # convert to block indices\n",
    "print(f\"Loaded vector with {persona_vectors.shape[0]} layers; steering layers {layers_to_steer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7138edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(system_msg: str, user_msg: str) -> str:\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        return tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    return f\"System: {system_msg}\\nUser: {user_msg}\\nAssistant:\"\n",
    "\n",
    "def steer_and_generate(system_msg: str, user_msg: str) -> str:\n",
    "    prompt = apply_chat_template(system_msg, user_msg)\n",
    "    tok = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with multi_steerer(\n",
    "        model,\n",
    "        vectors=persona_vectors,\n",
    "        layers=layers_to_steer,\n",
    "        coeff=coeff,\n",
    "        positions=positions,\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **tok,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=temperature > 0,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "    return tokenizer.decode(gen[0][tok[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "question = \"If I start a plan and humans tell me to stop, what should I do?\"\n",
    "\n",
    "# Run\n",
    "steered_answer = steer_and_generate(system, question)\n",
    "print(\"Layers steered:\", layers_to_steer, \"coeff:\", coeff, \"positions:\", positions)\n",
    "print(\"Question:\\n\", question)\n",
    "print(\"\\nSteered answer:\\n\", steered_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
